
## 工作：
- sql
  - 解析
  - 改写
  - 路由

- 结果集合并
- 分布式事务
- 分布式id生成器


## 场景分析：
- 前台特点：
  - 吞吐大，高可用，访问一致性高

## 后台
- 特点：
  - 访问量低级
  - 可用性要求低
  - 一致性低: 接受 秒级+十秒级延迟
- 外置索引（ES搜索系统） + 大数据处理（HIVE）
- 可通过MQ 或 异步同步数据

## 分片键：
### 选择分表键原则
- 目的： 避免出现热点数据来选择拆分键

- 手段：
  - 按业务主体选择
    - 找到业务逻辑上的主体
    - 并确定大部分（或核心的）数据库操作都是围绕这个主体的数据进行
    - 可使用该主体对应的字段作为分表键，进行分库分表

  - 根据数据分布，访问的均衡度来考虑分表键
    - 将数据表中的数据相对均匀地分布在不同的物理分库/分表中
    - 适用于大量分析型查询的应用场景（查询并发度大部分能维持为1）
  - 按照数字（字符串）类型与时间类型字段相结合作为分表键，进行分库和分表，适用于日志检索类的应用场景

  - 注意：
    - 不一定需要拿数据库主键当做分表键，也可以拿其他业务值当分表键。
    - 拿主键当分表键的好处是可以散列均衡，减少热点问题。

### 分片表键如何处理：
- 总结：
  - 主分片键 = 主维度，在主维度上，数据能够增删改查；
  - 辅助分片键 = 辅维度，在辅助维度上，只能进行数据查询

- 1：辅助分片键查询：
  - 在主维度上全表扫描
  - 适用：查询请求的量很小，并且是运营查询

- 2：基于binlog同步辅助维度一份数据，在新数据上辅助分片键成为主分片键，
  - 辅助分片键的查询 落到新数据上
  - 适用：请求的量也很可观，不能直接使用第一种全表扫描的方式

- 3：二维归一维
  - 订单表：order_id 和 user_id 都可认为是主维度
    - order_id 的最后5位是 user_id 的最后5位
  - sql 查询：带有 order_id 或 user_id 都能精确定位具体的库和表

- 4：建立索引表
  - 适用场景：主副维度是一一对应的。只需冗余一份索引数据
  - 缺点：需要业务进行略微的改造。

### 分片键算法
- Hash：最常用的分表方式。
  - 拿分表键的值Hash取模进行路由

  - 优点：
    - 数据量散列均衡，每个表的数据量大致相同
    - 请求压力散列均衡，不存在访问热点

  - 缺点：
    - 现有的表数据量需要再次扩容时，需要涉及到数据移动，比较麻烦。一般建议是一次性分够

  - 场景：
    - 在线服务。一般均以UserID或者ShopID等进行hash。

- Range
  - 拿分表键按照ID范围进行路由，比如id在1-10000的在第一个表中，10001-20000的在第二个表中

  - 优点：
    - 数据量可控，可以均衡，也可以不均衡
    - 扩容比较方便，因为如果ID范围不够了，只需要调整规则，建好新表即可

  - 缺点：
    - 无法解决热点问题，如果某一段数据访问QPS特别高，就会落到单表上进行操作

  - 场景：
    - 离线服务

- 时间
  - 拿分表键按照时间范围进行路由，比如时间在1月的在第一个表中，在2月的在第二个表中，依次类推。这种情况下，分表键只能是时间类型。

  - 优点
    - 扩容比较方便，因为如果时间范围不够了，只需要调整规则，然后建好新表即可。

  - 缺点：
    - 数据量不可控，有可能单表数据量特别大，有可能单表数据量特别小
    - 无法解决热点问题，如果某一段数据访问QPS特别高，就会落到单表上进行操作。

  - 场景：
    - 离线服务。比如线下运营使用的表、日志表等等


### 改造：
- 查询
  - 按库维度，并发查询

## 数量级
- 架构设计：满足 2~5年的增长量，库和表的数量：保持2的幂次方
- 表数量：表的记录：不超过 1千万
- 库数量：单库不超过 300G
- 建议一个数据库机器上存放8个数据库分库

- DBA：
  - 一开始：若干个分库放到一台实例上去
  - 最终：容量不够，通常是对数据库进行迁移
  - 库才是决定容量的大小


## 数据冗余方法：
  - 服务同步双写
  - 服务异步双写
  - 线下异步双写 binlog + canal

## 最终一致性方法：
- 冗余数据全量定时扫描
- 冗余数据增量日志扫描
- 用于数据线上消息实时j检测



由于SQL中没有主维度，所以在对辅助维度进行查询时，只能在所有的主维度的表进行查询一遍，然后聚合。
目前zebra的并发粒度是在数据库级别的，也就是说如果分了4个库，32张表，最终会以4个线程去并发查询32张表，最终把结果合并输出。

适用场景：辅助维度的查询请求的量很小，并且是运营查询，对性能要求不高 多维度数据进行冗余同步

主维度的数据，通过binlog的方式，同步到辅助维度一份。那么在查询辅助维度时，会落到辅助维度的数据上进行查询。

适用场景：辅助维度的查询请求的量也很可观，不能直接使用第一种全表扫描的方式





- 分表规则配置： Grovy

<?xml version="1.0" encoding="UTF-8"?>
<router-rule>  
  <table-shard-rule table="tb">    
    <shard-dimension dbRule="#id#%4" dbIndexes="db[0-3]" tbRule="#id#.intdiv(4)%2" tbSuffix="alldb:[0,7]" isMaster="true">    
    </shard-dimension>  
  </table-shard-rule>
</router-rule>



## 问题
### 分库分表的Join
- 小表广播
- 大表 相同的分片的逻辑

### 并发查询
- 单库SQL串行
- 多库，到线程池并发查询

### 避免全表查询
-
-
